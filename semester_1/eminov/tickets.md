### 1. Дискретная случайная величина. Способ задания. Математическое ожидание и дисперсия. Моделирование дискретной случайной величины.

#### **Дискретная случайная величина**
Дискретная случайная величина (СВ) — это величина, которая принимает конечное или счетное множество значений, каждое из которых имеет свою вероятность. Например, количество очков при броске кубика.

#### **Способы задания**
1. **Табличный способ:**
   Указывается список возможных значений $$x_i$$ и их вероятности $$p_i$$. Условие нормировки:  

   $$\sum_{i=1}^{n} p_i = 1, \quad p_i \geq 0.$$

   Пример:

$$X = 
\begin{cases} 
1, & P(X=1) = 0.2, \\
2, & P(X=2) = 0.5, \\
3, & P(X=3) = 0.3.
\end{cases}$$


2. **Формульный способ:**
   Указывается формула для вероятностей. Например, $$P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}$$, $$k = 0, 1, 2, \ldots$$ (распределение Пуассона).

3. **Графический способ:**
   Строится гистограмма, где по оси $$x$$ откладываются значения $$x_i$$, а по оси $$y$$ — вероятности $$p_i$$.

#### **Математическое ожидание**
Математическое ожидание $$\mathbb{E}[X]$$ показывает среднее значение случайной величины при большом числе испытаний:

$$\mathbb{E}[X] = \sum_{i=1}^{n} x_i p_i.$$

Пример: Если $$X = \{1, 2, 3\}$$, $$P(X) = \{0.2, 0.5, 0.3\}$$, то:

$$\mathbb{E}[X] = 1 \cdot 0.2 + 2 \cdot 0.5 + 3 \cdot 0.3 = 2.1.$$


#### **Дисперсия**
Дисперсия $$\text{Var}(X)$$ измеряет разброс значений случайной величины относительно её математического ожидания:

$$\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2,$$

где $$\mathbb{E}[X^2] = \sum_{i=1}^{n} x_i^2 p_i$$.

Пример: Для $$X = \{1, 2, 3\}$$, $$P(X) = \{0.2, 0.5, 0.3\}$$:

$$\mathbb{E}[X^2] = 1^2 \cdot 0.2 + 2^2 \cdot 0.5 + 3^2 \cdot 0.3 = 4.7.$$


$$\text{Var}(X) = 4.7 - (2.1)^2 = 0.39.$$


#### **Моделирование дискретной случайной величины**
Моделирование дискретной СВ выполняется методом обратной функции или методом прямого выбора.  

1. **Метод прямого выбора:**
   - Составляется интервальный ряд, где каждому значению $$x_i$$ соответствует вероятность $$p_i$$.
   - Генерируется случайное число $$r \in [0, 1)$$.
   - Находится значение $$x_i$$, соответствующее интервалу, в который попадает $$r$$.

   Пример: Для $$P(X=1)=0.2, P(X=2)=0.5, P(X=3)=0.3$$:
   - Интервалы: $$[0, 0.2), [0.2, 0.7), [0.7, 1)$$.
   - Если $$r = 0.35$$, то $$X = 2$$.

2. **Метод обратной функции:**
   - Считается функция распределения $$F(x) = P(X \leq x)$$.
   - Для случайного числа $$r \in [0, 1)$$ находим $$X = F^{-1}(r)$$.

#### **Пример на Python**

```python
import random

# Определение вероятностей
values = [1, 2, 3]
probabilities = [0.2, 0.5, 0.3]

# Генерация случайной величины
def generate_discrete_random(values, probabilities):
    cumulative = [sum(probabilities[:i+1]) for i in range(len(probabilities))]
    r = random.random()
    for i, p in enumerate(cumulative):
        if r < p:
            return values[i]

# Моделирование
simulated_values = [generate_discrete_random(values, probabilities) for _ in range(1000)]
print(f"Смоделированные значения: {simulated_values[:10]}")
```

---

### 2. Непрерывная случайная величина. Функция распределения. Плотность распределения. Математическое ожидание и дисперсия. Моделирование непрерывной случайной величины.

#### **Непрерывная случайная величина**
Непрерывная случайная величина (СВ) — это величина, которая принимает значения из непрерывного множества (например, интервалов на числовой прямой). Вероятность того, что СВ примет конкретное значение, равна нулю, но вероятность попадания в интервал может быть рассчитана.

#### **Функция распределения**
Функция распределения $$F(x)$$ описывает вероятность того, что СВ $$X$$ примет значение, меньшее или равное $$x$$:

$$F(x) = P(X \leq x).$$

Основные свойства:
1. $$F(x)$$ неубывающая: $$F(x_1) \leq F(x_2)$$, если $$x_1 < x_2$$.
2. $$F(x) \to 0$$ при $$x \to -\infty$$, $$F(x) \to 1$$ при $$x \to +\infty$$.
3. $$P(a \leq X \leq b) = F(b) - F(a)$$.

Пример: Для равномерного распределения $$X \sim U(a, b)$$,

$$F(x) = 
\begin{cases} 
0, & x < a, \\
\frac{x-a}{b-a}, & a \leq x \leq b, \\
1, & x > b.
\end{cases}$$


#### **Плотность распределения**
Функция плотности вероятности $$f(x)$$ связана с функцией распределения следующим образом:

$$f(x) = \frac{dF(x)}{dx}.$$

Свойства:
1. $$f(x) \geq 0$$.
2. $$\int_{-\infty}^{\infty} f(x) \, dx = 1$$.
3. $$P(a \leq X \leq b) = \int_{a}^{b} f(x) \, dx$$.

Пример: Для нормального распределения $$X \sim N(\mu, \sigma^2)$$:

$$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}.$$


#### **Математическое ожидание**
Математическое ожидание $$\mathbb{E}[X]$$ определяется как интеграл:

$$\mathbb{E}[X] = \int_{-\infty}^{\infty} x f(x) \, dx.$$

Пример: Для $$X \sim U(a, b)$$:

$$\mathbb{E}[X] = \int_{a}^{b} x \frac{1}{b-a} \, dx = \frac{a+b}{2}.$$


#### **Дисперсия**
Дисперсия $$\text{Var}(X)$$ измеряет разброс значений относительно математического ожидания:

$$\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2,$$

где

$$\mathbb{E}[X^2] = \int_{-\infty}^{\infty} x^2 f(x) \, dx.$$

Пример: Для $$X \sim U(a, b)$$:

$$\text{Var}(X) = \frac{(b-a)^2}{12}.$$


#### **Моделирование непрерывной случайной величины**

1. **Метод обратной функции распределения:**
   - Если известна функция распределения $$F(x)$$, генерируется случайное число $$r \in [0, 1)$$.
   - Значение $$X$$ находится как решение уравнения $$F(X) = r$$.

   Пример: Для $$X \sim U(0, 1)$$:

   $$F(x) = x, \quad x \in [0, 1].$$

   Тогда $$X = r$$.

2. **Метод преобразований:**
   Используются преобразования известных распределений. Например, для экспоненциального распределения $$X \sim \text{Exp}(\lambda)$$:

   $$X = -\frac{1}{\lambda} \ln(1 - r),$$

   где $$r \in [0, 1)$$.

3. **Метод отказов (Rejection Sampling):**
   - Генерируется значение $$Y$$ из простого распределения $$g(y)$$.
   - Принимается $$Y$$ с вероятностью $$\frac{f(Y)}{M g(Y)}$$, где $$M$$ — константа.

#### **Пример на Python**

```python
import random
import math

# Генерация случайной величины методом обратной функции
def generate_uniform(a, b):
    r = random.random()
    return a + (b - a) * r

# Генерация случайной величины по экспоненциальному закону
def generate_exponential(lmbda):
    r = random.random()
    return -math.log(1 - r) / lmbda

# Пример моделирования
uniform_values = [generate_uniform(0, 10) for _ in range(1000)]
exp_values = [generate_exponential(1.5) for _ in range(1000)]

print(f"Пример равномерных значений: {uniform_values[:5]}")
print(f"Пример экспоненциальных значений: {exp_values[:5]}")
```

---

### 3. Случайная величина, распределённая по равномерному закону.  
Функция распределения. Плотность распределения. Математическое ожидание и дисперсия. Моделирование равномерного распределения.

#### **Определение равномерного распределения**
Равномерное распределение $$X \sim U(a, b)$$ описывает случайную величину, которая с одинаковой вероятностью принимает значения в интервале $$[a, b]$$. Величина $$X$$ характеризуется:
- Интервалом $$[a, b]$$;
- Постоянной плотностью вероятности внутри этого интервала.

#### **Функция распределения**
Функция распределения $$F(x)$$ для равномерного распределения имеет вид:

$$F(x) = 
\begin{cases} 
0, & x < a, \\
\frac{x-a}{b-a}, & a \leq x \leq b, \\
1, & x > b.
\end{cases}$$


#### **Плотность распределения**
Функция плотности вероятности $$f(x)$$ равномерного распределения определяется как:

$$f(x) = 
\begin{cases} 
\frac{1}{b-a}, & a \leq x \leq b, \\
0, & \text{иначе}.
\end{cases}$$

Основные свойства:
1. Константная плотность вероятности $$f(x) = \frac{1}{b-a}$$ внутри интервала $$[a, b]$$.
2. $$\int_a^b f(x) \, dx = 1$$, то есть сумма всех вероятностей равна 1.

#### **Математическое ожидание**
Математическое ожидание $$\mathbb{E}[X]$$ для равномерного распределения:

$$\mathbb{E}[X] = \frac{a+b}{2}.$$

Это среднее значение интервала.

#### **Дисперсия**
Дисперсия $$\text{Var}(X)$$ равномерного распределения:

$$\text{Var}(X) = \frac{(b-a)^2}{12}.$$

Это мера разброса значений относительно среднего.

#### **Пример**
Для $$X \sim U(2, 8)$$:
- $$f(x) = \frac{1}{8-2} = \frac{1}{6}$$, если $$2 \leq x \leq 8$$.
- $$\mathbb{E}[X] = \frac{2+8}{2} = 5$$.
- $$\text{Var}(X) = \frac{(8-2)^2}{12} = \frac{36}{12} = 3$$.

#### **Моделирование равномерного распределения**

1. **Метод встроенной генерации:**
   Для генерации равномерно распределённых чисел используется стандартная функция генерации случайных чисел в большинстве языков программирования.

   Например, в Python:
   ```python
   import random
   # Генерация случайного числа из U(a, b)
   def generate_uniform(a, b):
       return random.uniform(a, b)
   
   # Пример
   a, b = 2, 8
   uniform_values = [generate_uniform(a, b) for _ in range(10)]
   print(uniform_values)
   ```

2. **Метод преобразования:**
   Если $$r \in [0, 1)$$ — случайная величина, равномерно распределённая на $$[0, 1)$$, то случайная величина $$X$$ из $$U(a, b)$$ вычисляется как:

   $$X = a + (b-a) \cdot r.$$


#### **Пример расчёта плотности и функции распределения**
1. Пусть $$X \sim U(2, 8)$$. Требуется найти:
   - $$P(3 \leq X \leq 5)$$.
   - Значение $$F(4)$$.

Решение:
- $$f(x) = \frac{1}{8-2} = \frac{1}{6}$$.
- $$P(3 \leq X \leq 5) = \int_3^5 \frac{1}{6} dx = \frac{1}{6} \cdot (5-3) = \frac{2}{6} = \frac{1}{3}$$.
- $$F(x) = \frac{x-2}{6}, \quad a \leq x \leq b$$. Тогда $$F(4) = \frac{4-2}{6} = \frac{2}{6} = \frac{1}{3}$$.

#### **График распределения**
Построение графиков плотности и функции распределения помогает визуализировать равномерное распределение. 

```python
import numpy as np
import matplotlib.pyplot as plt

# Параметры равномерного распределения
a, b = 2, 8

# Плотность распределения
x = np.linspace(0, 10, 500)
f = np.where((x >= a) & (x <= b), 1 / (b - a), 0)

# Функция распределения
F = np.where(x < a, 0, np.where(x > b, 1, (x - a) / (b - a)))

# Построение графиков
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(x, f, label='Плотность распределения f(x)')
plt.title("Плотность распределения")
plt.xlabel("x")
plt.ylabel("f(x)")
plt.grid()
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(x, F, label='Функция распределения F(x)', color='orange')
plt.title("Функция распределения")
plt.xlabel("x")
plt.ylabel("F(x)")
plt.grid()
plt.legend()

plt.tight_layout()
plt.show()
```

---

### 4. Случайная величина, распределённая по показательному закону.  
Функция распределения. Плотность распределения. Математическое ожидание и дисперсия. Моделирование показательного распределения.

#### **Определение показательного распределения**
Показательное распределение $$X \sim \text{Exp}(\lambda)$$ описывает случайную величину $$X$$, характеризующую время до наступления первого события в потоке, где события происходят с интенсивностью $$\lambda > 0$$. Оно применяется в моделировании процессов, где события происходят непрерывно и независимо.

#### **Плотность распределения**
Функция плотности вероятности $$f(x)$$ для показательного распределения:

$$f(x) = 
\begin{cases} 
\lambda e^{-\lambda x}, & x \geq 0, \\
0, & x < 0.
\end{cases}$$

Свойства:
1. $$f(x) > 0$$ при $$x \geq 0$$.
2. $$\int_0^\infty f(x) \, dx = 1$$, что подтверждает корректность распределения вероятностей.

#### **Функция распределения**
Функция распределения $$F(x)$$ показывает вероятность того, что случайная величина $$X$$ примет значение, меньшее или равное $$x$$:

$$F(x) = 
\begin{cases} 
1 - e^{-\lambda x}, & x \geq 0, \\
0, & x < 0.
\end{cases}$$


#### **Математическое ожидание**
Математическое ожидание $$\mathbb{E}[X]$$ равно:

$$\mathbb{E}[X] = \frac{1}{\lambda}.$$

Оно описывает среднее время между двумя событиями.

#### **Дисперсия**
Дисперсия $$\text{Var}(X)$$ равна:

$$\text{Var}(X) = \frac{1}{\lambda^2}.$$

Это мера разброса времени между событиями.

#### **Пример**
Пусть $$X \sim \text{Exp}(2)$$, то есть интенсивность $$\lambda = 2$$:
- $$f(x) = 2 e^{-2x}, \, x \geq 0$$.
- $$F(x) = 1 - e^{-2x}, \, x \geq 0$$.
- $$\mathbb{E}[X] = \frac{1}{2} = 0.5$$.
- $$\text{Var}(X) = \frac{1}{2^2} = 0.25$$.

#### **Моделирование показательного распределения**

1. **Метод преобразования случайной величины:**
   Если $$U \sim U(0, 1)$$ — случайная величина с равномерным распределением на $$[0, 1)$$, то случайная величина $$X$$ с показательной плотностью $$\lambda$$ может быть получена как:

   $$X = -\frac{\ln(U)}{\lambda}.$$


2. **Пример на Python:**
   ```python
   import numpy as np

   # Параметры распределения
   lambda_value = 2

   # Генерация случайных чисел
   def generate_exponential(lambda_value, size=10):
       return np.random.exponential(scale=1/lambda_value, size=size)

   # Пример
   random_values = generate_exponential(lambda_value, size=10)
   print("Случайные числа из показательного распределения:", random_values)
   ```

#### **График плотности и функции распределения**
Графическое представление плотности и функции распределения помогает лучше понять свойства показательного закона.

```python
import numpy as np
import matplotlib.pyplot as plt

# Параметры распределения
lambda_value = 2
x = np.linspace(0, 3, 500)

# Плотность распределения
f = lambda_value * np.exp(-lambda_value * x)

# Функция распределения
F = 1 - np.exp(-lambda_value * x)

# Построение графиков
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(x, f, label='Плотность распределения f(x)')
plt.title("Плотность распределения")
plt.xlabel("x")
plt.ylabel("f(x)")
plt.grid()
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(x, F, label='Функция распределения F(x)', color='orange')
plt.title("Функция распределения")
plt.xlabel("x")
plt.ylabel("F(x)")
plt.grid()
plt.legend()

plt.tight_layout()
plt.show()
```

#### **Пример задачи**
Для $$X \sim \text{Exp}(1.5)$$ требуется найти:
1. Вероятность $$P(X \leq 1)$$.
2. Вероятность $$P(1 \leq X \leq 2)$$.

Решение:
1. Используем функцию распределения:

   $$P(X \leq 1) = F(1) = 1 - e^{-1.5 \cdot 1} \approx 1 - 0.2231 = 0.7769.$$

2. Используем разность функций распределения:

   $$P(1 \leq X \leq 2) = F(2) - F(1).$$

   Для $$F(2) = 1 - e^{-1.5 \cdot 2} \approx 1 - 0.0498 = 0.9502$$, тогда:

   $$P(1 \leq X \leq 2) = 0.9502 - 0.7769 = 0.1733.$$


---

### 5. Нормальное распределение. Основные свойства. Моделирование нормального распределения.

#### **Определение нормального распределения**

Нормальное (гауссово) распределение описывает случайную величину, значения которой концентрируются вокруг среднего значения $$\mu$$ и имеют симметричное распределение. Оно задаётся двумя параметрами:
- **Математическое ожидание** ($$\mu$$): среднее значение случайной величины.
- **Дисперсия** ($$\sigma^2$$): мера разброса вокруг среднего.

Случайная величина $$X$$ с нормальным распределением обозначается как:

$$X \sim N(\mu, \sigma^2).$$


#### **Функция плотности вероятности (PDF)**

Плотность нормального распределения имеет вид:

$$f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}, \quad x \in \mathbb{R}.$$


Свойства:
1. Функция симметрична относительно $$\mu$$.
2. Максимум функции достигается в точке $$x = \mu$$.
3. Интеграл функции равен 1:

   $$\int_{-\infty}^\infty f(x) \, dx = 1.$$


#### **Функция распределения (CDF)**

Функция распределения нормальной случайной величины $$F(x)$$ равна:

$$F(x) = P(X \leq x) = \int_{-\infty}^x f(t) \, dt.$$


Значение $$F(x)$$ вычисляется численно или с использованием таблиц стандартного нормального распределения.

#### **Стандартное нормальное распределение**

Если $$\mu = 0$$ и $$\sigma^2 = 1$$, то распределение называется стандартным нормальным и обозначается $$Z \sim N(0, 1)$$. Его плотность:

$$\phi(z) = \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}}.$$


Для перехода от общего нормального распределения $$X \sim N(\mu, \sigma^2)$$ к стандартному:

$$Z = \frac{X - \mu}{\sigma}.$$


#### **Свойства нормального распределения**

1. **Симметричность:** Плотность симметрична относительно $$\mu$$.
2. **Мода, медиана и среднее совпадают:** Все они равны $$\mu$$.
3. **Вероятности в пределах стандартных отклонений:**
   - $$P(\mu - \sigma \leq X \leq \mu + \sigma) \approx 68\%$$,
   - $$P(\mu - 2\sigma \leq X \leq \mu + 2\sigma) \approx 95\%$$,
   - $$P(\mu - 3\sigma \leq X \leq \mu + 3\sigma) \approx 99.7\%$$.
4. **Замкнутость:** Линейная комбинация нормальных величин также имеет нормальное распределение.

#### **Математическое ожидание и дисперсия**

1. Математическое ожидание:

   $$\mathbb{E}[X] = \mu.$$

2. Дисперсия:

   $$\text{Var}(X) = \sigma^2.$$


#### **Пример**

Пусть $$X \sim N(3, 4)$$ ($$\mu = 3, \sigma^2 = 4$$), тогда:
1. Функция плотности:

   $$f(x) = \frac{1}{\sqrt{2 \pi \cdot 4}} e^{-\frac{(x - 3)^2}{8}}.$$

2. Среднее значение:

   $$\mathbb{E}[X] = 3.$$

3. Дисперсия:

   $$\text{Var}(X) = 4.$$

4. Вероятность $$P(X \leq 4)$$ вычисляется через стандартное нормальное распределение:

   $$Z = \frac{4 - 3}{\sqrt{4}} = 0.5.$$


#### **Моделирование нормального распределения**

1. **Метод Бокса-Мюллера:**
   Для генерации пары независимых случайных величин с нормальным распределением:

   $$Z_0 = \sqrt{-2 \ln(U_1)} \cos(2\pi U_2),$$


   $$Z_1 = \sqrt{-2 \ln(U_1)} \sin(2\pi U_2),$$

   где $$U_1, U_2 \sim U(0, 1)$$ — независимые равномерно распределённые случайные величины.

2. **Встроенные функции Python:**
   ```python
   import numpy as np

   # Параметры распределения
   mu = 3
   sigma = 2

   # Генерация случайных чисел
   random_values = np.random.normal(mu, sigma, size=10)
   print("Случайные числа из нормального распределения:", random_values)
   ```

#### **График плотности и функции распределения**

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Параметры нормального распределения
mu = 0
sigma = 1

# Диапазон значений
x = np.linspace(-4, 4, 500)

# Плотность и функция распределения
pdf = norm.pdf(x, mu, sigma)
cdf = norm.cdf(x, mu, sigma)

# Построение графиков
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(x, pdf, label='Плотность распределения (PDF)')
plt.title("Плотность нормального распределения")
plt.xlabel("x")
plt.ylabel("f(x)")
plt.grid()
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(x, cdf, label='Функция распределения (CDF)', color='orange')
plt.title("Функция распределения")
plt.xlabel("x")
plt.ylabel("F(x)")
plt.grid()
plt.legend()

plt.tight_layout()
plt.show()
```

#### **Пример задачи**

Для $$X \sim N(5, 1)$$:
1. Найти вероятность $$P(4 \leq X \leq 6)$$.
2. Вычислить $$P(X > 7)$$.

Решение:
1. Используем стандартное нормальное распределение:

   $$Z_1 = \frac{4 - 5}{1} = -1, \quad Z_2 = \frac{6 - 5}{1} = 1.$$

   Тогда $$P(4 \leq X \leq 6) = P(-1 \leq Z \leq 1) \approx 0.6827$$.

2. Для $$P(X > 7)$$:

   $$Z = \frac{7 - 5}{1} = 2.$$

   Тогда $$P(X > 7) = 1 - F(2) \approx 1 - 0.9772 = 0.0228$$.

---

### 6. Системы массового обслуживания (СМО). Основные понятия. Математическое моделирование СМО. Граф состояний. Уравнения Колмогорова.

#### **Основные понятия СМО**

**Система массового обслуживания (СМО)** — это математическая модель, описывающая процесс обслуживания заявок (требований) в системе с ограниченными ресурсами. 

СМО используется для анализа очередей и обслуживания в реальных системах: телекоммуникационные сети, банковские системы, производственные линии.

##### **Элементы СМО:**
1. **Поток заявок:**
   - Заявки поступают в систему с определённой интенсивностью $$\lambda$$ (среднее число заявок в единицу времени).
   - Поток может быть:
     - **Пуассоновский** (равномерный и случайный).
     - **Регулярный** (заявки поступают с постоянной частотой).

2. **Очередь:**
   - Место ожидания заявок перед обслуживанием.
   - Характеристики очереди:
     - Максимальная длина очереди ($$L_{\text{max}}$$).
     - Поведение в случае переполнения очереди (отказы, прерывания).

3. **Обслуживающие каналы:**
   - Количество каналов обслуживания ($$n$$).
   - Интенсивность обслуживания одного канала $$\mu$$ (среднее число обслуженных заявок в единицу времени).

4. **Дисциплина обслуживания:**
   - **FIFO** (первый пришёл — первый ушёл).
   - **LIFO** (последний пришёл — первый ушёл).
   - **Приоритетное обслуживание.**

##### **Параметры системы:**
- **Нагрузка на систему:** $$\rho = \frac{\lambda}{n \mu}$$, где $$n$$ — количество каналов.
- **Вероятность отказа:** вероятность того, что заявка не попадёт в систему из-за занятых каналов.
- **Среднее время ожидания** и **длина очереди**.

##### **Классификация СМО (нотация Джексона):**
Обозначение $$A/B/n/K$$:
1. $$A$$ — характер поступления заявок (например, $$M$$ — пуассоновский поток).
2. $$B$$ — распределение времени обслуживания (например, $$M$$ — экспоненциальное распределение).
3. $$n$$ — число обслуживающих каналов.
4. $$K$$ — максимальная длина очереди (по умолчанию бесконечность).

Пример: $$M/M/1$$ — одноканальная система с пуассоновским потоком заявок и экспоненциальным распределением времени обслуживания.

#### **Математическое моделирование СМО**

Модель основывается на анализе вероятностных характеристик системы, таких как:
- **Вероятность нахождения системы в состоянии $$n$$:** $$P_n$$ — вероятность того, что в системе $$n$$ заявок.
- **Средняя длина очереди ($$L$$):**
  
$$L = \sum_{n=0}^\infty n P_n.$$

- **Среднее время ожидания ($$W$$):**
  
$$W = \frac{L}{\lambda_{\text{eff}}}, \quad \lambda_{\text{eff}} = \lambda(1 - P_{\text{отказ}}).$$

#### **Граф состояний**

Граф состояний используется для визуализации возможных состояний СМО и переходов между ними. 

Пример: $$M/M/1$$ с интенсивностью поступления заявок $$\lambda$$ и обслуживания $$\mu$$:
- Состояние $$n$$ — в системе $$n$$ заявок.
- Переходы:
  - $$n \to n+1$$ (поступление заявки) с вероятностью $$\lambda \Delta t$$.
  - $$n \to n-1$$ (обслуживание заявки) с вероятностью $$\mu \Delta t$$.

Граф:

$$0 \overset{\lambda}{\to} 1 \overset{\lambda}{\to} 2 \overset{\lambda}{\to} \dots$$


$$\dots \overset{\mu}{\to} 2 \overset{\mu}{\to} 1 \overset{\mu}{\to} 0.$$


#### **Уравнения Колмогорова**

Для описания вероятностной модели используются уравнения Колмогорова (системы дифференциальных уравнений):

$$\frac{dP_n(t)}{dt} = \lambda P_{n-1}(t) + \mu P_{n+1}(t) - (\lambda + \mu) P_n(t), \quad n \geq 1.$$

Для состояния $$n = 0$$:

$$\frac{dP_0(t)}{dt} = \mu P_1(t) - \lambda P_0(t).$$


**Стационарное состояние:** вероятности не зависят от времени ($$\frac{dP_n}{dt} = 0$$):

$$P_n = \frac{\rho^n}{n!} P_0, \quad P_0 = \left( \sum_{n=0}^\infty \frac{\rho^n}{n!} \right)^{-1}.$$


Где $$\rho = \frac{\lambda}{\mu}$$ — нагрузка системы.

#### **Пример расчёта $$M/M/1$$:**

1. Интенсивность поступления заявок $$\lambda = 5$$ заявок/час.
2. Интенсивность обслуживания $$\mu = 10$$ заявок/час.
3. Нагрузка системы:

   $$\rho = \frac{\lambda}{\mu} = 0.5.$$

4. Вероятность занятости системы:

   $$P_{\text{занято}} = \rho = 0.5.$$

5. Вероятность пустой системы:

   $$P_0 = 1 - \rho = 0.5.$$

6. Средняя длина очереди:

   $$L = \frac{\rho^2}{1 - \rho} = \frac{0.5^2}{1 - 0.5} = 0.5.$$

7. Среднее время ожидания:

   $$W = \frac{L}{\lambda} = \frac{0.5}{5} = 0.1 \, \text{часа}.$$


#### **Практическое применение**

- Оптимизация числа обслуживающих каналов.
- Планирование ресурсов (оборудования, сотрудников).
- Уменьшение времени ожидания и повышения производительности системы.

---

### 7. Процессы гибели и размножения. Граф состояний и уравнения Колмогорова.

#### **Процессы гибели и размножения**

Процессы гибели и размножения — это типы стохастических процессов, описывающих динамику численности объектов (особей, частиц, заявок) во времени. Используются для моделирования таких систем, как популяционная динамика, очереди, эпидемии и радиоактивный распад.

##### **Основные характеристики:**

1. **Состояния системы:**
   - Система находится в состоянии $$n$$, если в текущий момент времени в ней $$n$$ объектов.

2. **Интенсивности переходов:**
   - $$\lambda_n$$ — интенсивность размножения (переход из состояния $$n$$ в $$n+1$$).
   - $$\mu_n$$ — интенсивность гибели (переход из состояния $$n$$ в $$n-1$$).

3. **Типы процессов:**
   - Чистый процесс размножения ($$\mu_n = 0$$): размер системы только увеличивается.
   - Чистый процесс гибели ($$\lambda_n = 0$$): размер системы только уменьшается.
   - Комбинированный процесс ($$\lambda_n > 0$$, $$\mu_n > 0$$).

4. **Вероятности состояний:**
   - $$P_n(t)$$ — вероятность того, что в момент времени $$t$$ система находится в состоянии $$n$$.

#### **Граф состояний**

Граф состояний процессов гибели и размножения представляет возможные состояния системы и вероятности переходов между ними.

- Вершины графа — состояния $$n = 0, 1, 2, \dots$$.
- Рёбра графа — переходы между состояниями с интенсивностями $$\lambda_n$$ (вверх) и $$\mu_n$$ (вниз).

Пример графа для комбинированного процесса:

$$0 \overset{\lambda_0}{\to} 1 \overset{\lambda_1}{\to} 2 \overset{\lambda_2}{\to} \dots$$


$$\dots \overset{\mu_2}{\to} 2 \overset{\mu_1}{\to} 1 \overset{\mu_0}{\to} 0.$$


#### **Уравнения Колмогорова**

Уравнения Колмогорова описывают эволюцию вероятностей состояний системы во времени. 

Для процессов гибели и размножения дифференциальные уравнения имеют вид:

$$\frac{dP_n(t)}{dt} = \lambda_{n-1}P_{n-1}(t) + \mu_{n+1}P_{n+1}(t) - (\lambda_n + \mu_n)P_n(t),$$

где:
- $$P_{n-1}(t)$$ и $$P_{n+1}(t)$$ — вероятности перехода в состояние $$n$$ из соседних состояний.
- $$\lambda_n + \mu_n$$ — суммарная интенсивность выхода из состояния $$n$$.

Для $$n = 0$$ и $$n = N$$ (если $$N$$ — конечное максимальное состояние):

$$\frac{dP_0(t)}{dt} = \mu_1 P_1(t) - \lambda_0 P_0(t),$$


$$\frac{dP_N(t)}{dt} = \lambda_{N-1} P_{N-1}(t) - \mu_N P_N(t).$$


#### **Стационарное состояние**

В стационарном состоянии вероятности состояний не зависят от времени ($$\frac{dP_n(t)}{dt} = 0$$):

$$\lambda_{n-1}P_{n-1} + \mu_{n+1}P_{n+1} = (\lambda_n + \mu_n)P_n.$$


Рекуррентная формула для вероятностей:

$$P_n = P_0 \prod_{i=1}^n \frac{\lambda_{i-1}}{\mu_i}.$$


Сумма вероятностей всех состояний равна единице:

$$\sum_{n=0}^\infty P_n = 1.$$


#### **Пример расчёта**

Рассмотрим процесс с постоянными интенсивностями:
- $$\lambda_n = \lambda$$, $$\mu_n = \mu$$.

Стационарное распределение:

$$P_n = P_0 \left(\frac{\lambda}{\mu}\right)^n.$$


Нормировка:

$$P_0 = \left( \sum_{n=0}^\infty \left(\frac{\lambda}{\mu}\right)^n \right)^{-1}.$$

Если $$\frac{\lambda}{\mu} < 1$$ (сходимость ряда):

$$P_0 = 1 - \frac{\lambda}{\mu}.$$


Вероятности состояний:

$$P_n = \left(1 - \frac{\lambda}{\mu}\right)\left(\frac{\lambda}{\mu}\right)^n.$$


#### **Применение процессов гибели и размножения**

1. **Биология:**
   - Моделирование популяций (размножение особей и их вымирание).
2. **Физика:**
   - Радиоактивный распад (чистый процесс гибели).
3. **Теория очередей:**
   - Потоки заявок в системах массового обслуживания.
4. **Эпидемиология:**
   - Распространение инфекций (размножение) и выздоровление (гибель).

---

### 8. Многоканальная система массового обслуживания с отказами. Задача 7.8.

#### **Многоканальная система массового обслуживания с отказами**

Многоканальная система массового обслуживания (СМО) с отказами — это система, в которой:
1. Имеется $$N$$ каналов обслуживания.
2. Все каналы обслуживают заявки независимо друг от друга.
3. Если заявка поступает, когда все каналы заняты, то она сразу отклоняется (отказ).
4. Очередь отсутствует.

#### **Основные параметры системы**

1. **Параметры входного потока заявок:**
   - Заявки поступают в систему согласно потоку Пуассона с интенсивностью $$\lambda$$ (среднее количество заявок в единицу времени).

2. **Обслуживание заявок:**
   - Время обслуживания имеет экспоненциальное распределение с параметром $$\mu$$ (среднее число обслуженных заявок одним каналом в единицу времени).

3. **Число каналов:**
   - Система имеет $$N$$ каналов.

4. **Загрузка канала:**
   - Отношение интенсивности поступления заявок к интенсивности обслуживания одного канала: $$\rho = \frac{\lambda}{\mu}$$.

#### **Вероятностное описание**

Состояние системы определяется числом занятых каналов $$k$$, где $$k \in [0, N]$$.

1. **Вероятности состояний:**
   - $$P_k$$ — вероятность того, что в системе занято $$k$$ каналов.
   - Для многоканальной СМО с отказами используется стационарное распределение вероятностей.

2. **Формула Эрланга:**
   - Вероятность отказа (т.е. все каналы заняты, заявка отклоняется):
  
   $$P_{отказа} = P_N = \frac{\frac{\rho^N}{N!}}{\sum_{k=0}^N \frac{\rho^k}{k!}}.$$
   
   - Здесь $$\rho = \frac{\lambda}{\mu}$$ — загрузка одного канала.

3. **Пропускная способность системы:**
   - Часть заявок, обслуживаемых системой:
  
   $$P_{обслуживания} = 1 - P_{отказа}.$$

4. **Среднее число занятых каналов:**

   $$L = \rho \cdot P_{обслуживания}.$$


#### **Характеристики эффективности**

1. **Интенсивность обслуженных заявок:**

   $$\lambda_{обслуживания} = \lambda \cdot P_{обслуживания}.$$


2. **Коэффициент занятости каналов:**

   $$K = \frac{L}{N}.$$


#### **Граф состояний**

Граф описывает все возможные состояния системы ($$0, 1, 2, \dots, N$$) и переходы между ними:
- Переход из $$k$$ в $$k+1$$ с интенсивностью $$\lambda$$ (заявка поступила).
- Переход из $$k$$ в $$k-1$$ с интенсивностью $$k \cdot \mu$$ (обслуживание завершилось).

Граф:

$$0 \overset{\lambda}{\to} 1 \overset{\lambda}{\to} 2 \dots \overset{\lambda}{\to} N,$$


$$N \overset{N\mu}{\to} N-1 \overset{(N-1)\mu}{\to} \dots \overset{\mu}{\to} 0.$$


#### **Пример расчёта**

Пусть:
- $$N = 3$$ (три канала),
- $$\lambda = 6$$ заявок/час,
- $$\mu = 2$$ заявок/час на канал.

1. Загрузка системы:

   $$\rho = \frac{\lambda}{\mu} = \frac{6}{2} = 3.$$


2. Стационарные вероятности:
   Используем формулу Эрланга:

   $$P_k = \frac{\frac{\rho^k}{k!}}{\sum_{j=0}^N \frac{\rho^j}{j!}}, \quad k = 0, 1, \dots, N.$$

   Считаем:

   $$P_0 = \frac{1}{\frac{\rho^0}{0!} + \frac{\rho^1}{1!} + \frac{\rho^2}{2!} + \frac{\rho^3}{3!}},$$


   $$P_0 = \frac{1}{1 + 3 + \frac{9}{2} + \frac{27}{6}} = \frac{1}{1 + 3 + 4.5 + 4.5} = \frac{1}{13}.$$



   $$P_3 = \frac{\frac{\rho^3}{3!}}{\sum_{j=0}^N \frac{\rho^j}{j!}} = \frac{\frac{27}{6}}{13} = \frac{4.5}{13}.$$


3. Вероятность отказа:

   $$P_{отказа} = P_3 = \frac{4.5}{13} \approx 0.346.$$


4. Пропускная способность:

   $$P_{обслуживания} = 1 - P_{отказа} = 1 - 0.346 = 0.654.$$


5. Интенсивность обслуженных заявок:

   $$\lambda_{обслуживания} = \lambda \cdot P_{обслуживания} = 6 \cdot 0.654 \approx 3.924.$$


6. Среднее число занятых каналов:

   $$L = \rho \cdot P_{обслуживания} = 3 \cdot 0.654 \approx 1.962.$$


---

### 9. Методы нелинейной оптимизации. Экстремум функции двух переменных.

#### **Нелинейная оптимизация**

**Нелинейная оптимизация** — это процесс нахождения максимума или минимума целевой функции, которая может быть нелинейной, с учётом ограничений (или без них).

#### **Основные понятия**

1. **Целевая функция:**

   $$f(x, y)$$

   где $$x$$ и $$y$$ — переменные.

2. **Ограничения (если имеются):**
   - **Равенства:**
  
   $$g_i(x, y) = 0, \quad i = 1, \dots, m.$$
   
   - **Неравенства:**
  
   $$h_j(x, y) \leq 0, \quad j = 1, \dots, n.$$
   

3. **Критическая точка:**
   Точка $$(x^*, y^*)$$, где частные производные функции $$f(x, y)$$ равны нулю:

   $$\frac{\partial f}{\partial x} = 0, \quad \frac{\partial f}{\partial y} = 0.$$


4. **Гессиан:**
   Вторые частные производные, записанные в виде матрицы:

$$H(f) =
\begin{pmatrix}
\frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\
\frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2}
\end{pmatrix}.$$


#### **Условия экстремума функции двух переменных**

1. **Необходимые условия:**
   Если $$f(x, y)$$ имеет экстремум в точке $$(x^*, y^*)$$, то:

   $$\frac{\partial f}{\partial x} = 0, \quad \frac{\partial f}{\partial y} = 0.$$


2. **Достаточные условия (с использованием Гессиана):**
   Проверяется определённость матрицы Гессиана $$H(f)$$ в критической точке:
   - **Если $$H(f)$$ положительно определённая**, то $$f(x, y)$$ имеет локальный минимум.
   - **Если $$H(f)$$ отрицательно определённая**, то $$f(x, y)$$ имеет локальный максимум.
   - **Если $$H(f)$$ меняет знак**, то $$f(x, y)$$ имеет седловую точку.

#### **Методы поиска экстремумов**

1. **Градиентные методы:**
   Используются для минимизации или максимизации функции. Обновление параметров происходит по направлению антиградиента (или градиента):

   $$x_{k+1} = x_k - \alpha \nabla f(x_k),$$

   где $$\alpha$$ — шаг оптимизации.

2. **Метод Ньютона:**
   Используется для нахождения критических точек. Уравнение обновления:

   $$x_{k+1} = x_k - [H(f)]^{-1} \nabla f(x_k),$$

   где $$H(f)$$ — Гессиан.

3. **Методы численной оптимизации:**
   - Метод сопряжённых градиентов.
   - Метод Лагранжа для задач с ограничениями.

#### **Пример задачи**

Найдём экстремум функции:

$$f(x, y) = x^2 + y^2 - 4x - 6y + 13.$$


1. **Частные производные:**

   $$\frac{\partial f}{\partial x} = 2x - 4, \quad \frac{\partial f}{\partial y} = 2y - 6.$$


2. **Равенства для критической точки:**

   $$2x - 4 = 0 \quad \Rightarrow \quad x = 2,$$


   $$2y - 6 = 0 \quad \Rightarrow \quad y = 3.$$


3. **Критическая точка:**

$$(x*, y*) = (2, 3).$$

4. **Гессиан:**

$$H(f) =
\begin{pmatrix}
\frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\
\frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2}
\end{pmatrix} = 
\begin{pmatrix}
2 & 0 \\
0 & 2
\end{pmatrix}.$$


5. **Определённость Гессиана:**
   Все собственные значения $$\lambda = 2 > 0$$. Следовательно, $$H(f)$$ положительно определённая.

6. **Вывод:**
   Функция $$f(x, y)$$ имеет локальный минимум в точке $$(2, 3)$$. Значение минимума:

   $$f(2, 3) = 2^2 + 3^2 - 4 \cdot 2 - 6 \cdot 3 + 13 = 4 + 9 - 8 - 18 + 13 = 0.$$


#### **Вывод**

Методы нелинейной оптимизации позволяют решать задачи нахождения экстремума функций с двумя переменными как в аналитической, так и в численной форме. Использование Гессиана и градиента помогает классифицировать точки как минимум, максимум или седловую точку.

---

### 10. Задача линейного программирования. Графический метод решения задачи линейного программирования.

#### **Задача линейного программирования (ЗЛП)**

**Линейное программирование (ЛП)** — это раздел математического программирования, в котором целевая функция и ограничения представлены линейными уравнениями или неравенствами.

#### **Общая формулировка задачи**

1. **Целевая функция**:

   $$Z = c_1x_1 + c_2x_2 + \dots + c_nx_n \to \max \text{ или } \min,$$

   где $$c_i$$ — коэффициенты целевой функции, $$x_i$$ — переменные.

2. **Система ограничений**:

$$\begin{aligned}
a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n &\leq b_1, \\
a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n &\leq b_2, \\
&\vdots \\
a_{m1}x_1 + a_{m2}x_2 + \dots + a_{mn}x_n &\leq b_m,
\end{aligned}$$

   где $$a_{ij}$$ и $$b_i$$ — заданные коэффициенты.

3. **Неотрицательность переменных**:

   $$x_i \geq 0, \quad i = 1, 2, \dots, n.$$


#### **Графический метод решения**

**Графический метод** применяется для задач с двумя переменными ($$n = 2$$). Он заключается в нахождении оптимального решения путём визуализации области допустимых решений и целевой функции.

#### **Этапы решения**

1. **Постановка задачи.**
   Задать целевую функцию $$Z = c_1x_1 + c_2x_2$$ и систему ограничений.

2. **Построение области допустимых решений.**
   Каждое ограничение $$a_{11}x_1 + a_{12}x_2 \leq b_1$$ строится как прямая на плоскости. Область допустимых решений — пересечение полуплоскостей, определяемых ограничениями.

3. **Определение угловых точек области.**
   Угловые точки определяются как пересечения прямых, задающих ограничения.

4. **Подстановка угловых точек в целевую функцию.**
   Найти значение целевой функции в каждой угловой точке области допустимых решений.

5. **Выбор оптимального решения.**
   Оптимальное решение соответствует максимуму (или минимуму) значения целевой функции среди допустимых точек.

#### **Пример задачи линейного программирования (исправленный вариант)**

**Постановка задачи:**
Максимизировать целевую функцию:

$$Z = 3x_1 + 2x_2$$

при следующих ограничениях:

$$\begin{aligned}
x_1 + x_2 &\leq 6, \\
x_1 &\leq 4, \\
x_2 &\leq 3, \\
x_1, x_2 &\geq 0.
\end{aligned}$$


### **Решение:**

1. **Построение графиков ограничений:**

   - $$x_1 + x_2 = 6$$: Это прямая, проходящая через точки $$(6, 0)$$ и $$(0, 6)$$.
   - $$x_1 = 4$$: Это вертикальная прямая через точку $$(4, 0)$$.
   - $$x_2 = 3$$: Это горизонтальная прямая через точку $$(0, 3)$$.
   - Ограничение $$x_1, x_2 \geq 0$$ накладывает условие, что область допустимых решений находится в первой четверти координатной плоскости (все переменные неотрицательны).

2. **Определение области допустимых решений:**
   Область допустимых решений — это пересечение полуплоскостей, ограниченных этими прямыми. Угловые точки, определяемые пересечением этих прямых, — это возможные кандидаты на решение задачи.

3. **Угловые точки области:**
   - Пересечение прямой $$x_1 + x_2 = 6$$ и прямой $$x_1 = 4$$: точка $$(4, 2)$$.
   - Пересечение прямой $$x_1 + x_2 = 6$$ и прямой $$x_2 = 3$$: точка $$(3, 3)$$.
   - Пересечение прямой $$x_1 = 4$$ и оси $$x_2 = 0$$: точка $$(4, 0)$$.
   - Пересечение прямой $$x_2 = 3$$ и оси $$x_1 = 0$$: точка $$(0, 3)$$.

4. **Подставляем угловые точки в целевую функцию $$Z = 3x_1 + 2x_2$$:**
   - В точке $$(4, 0)$$: $$Z(4, 0) = 3 \times 4 + 2 \times 0 = 12$$.
   - В точке $$(4, 2)$$: $$Z(4, 2) = 3 \times 4 + 2 \times 2 = 16$$.
   - В точке $$(3, 3)$$: $$Z(3, 3) = 3 \times 3 + 2 \times 3 = 15$$.
   - В точке $$(0, 3)$$: $$Z(0, 3) = 3 \times 0 + 2 \times 3 = 6$$.

5. **Выбор оптимального решения:**
   Из полученных значений целевой функции $$Z$$, максимальное значение $$Z = 16$$ достигается в точке $$(4, 2)$$.

**Ответ:** Оптимальное решение задачи — точка $$(4, 2)$$ с максимальным значением целевой функции $$Z = 16$$.

#### **Вывод**

Графический метод наглядно показывает процесс решения задач линейного программирования с двумя переменными. Основная идея — выделить область допустимых решений, определить её вершины и вычислить значение целевой функции для каждой из них, чтобы найти оптимальное решение.
